{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypQ83vcuWaJc",
        "outputId": "8479e4a6-b378-49f1-a02b-a8accf9d0014"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDYaNwfmWf1n",
        "outputId": "ddfdeea7-c54d-4b4c-b08c-e00fa01304a9"
      },
      "outputs": [],
      "source": [
        "%cd drive/MyDrive/dataset/feedback_effectiveness/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aBSB2aWWhrJ",
        "outputId": "ccca1ae3-d39e-414f-e81a-7bd7a771c660"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install tokenizers\n",
        "!pip install sentencepiece\n",
        "!pip install protobuf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkLrXAV8Wjel"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam, AdamW\n",
        "from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, CosineAnnealingWarmRestarts\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModel\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSG0K8soaMhs"
      },
      "outputs": [],
      "source": [
        "class global_CFG:\n",
        "    path_to_train_csv = './train_stratified.csv'\n",
        "    path_to_validation_csv = './validation_stratified.csv'\n",
        "\n",
        "    w_xsmall = 0.14\n",
        "    w_small = 0.86\n",
        "    w_base = 0.0\n",
        "\n",
        "    infer_batch_size = 16\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rN8lVcFMaT6S"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv(global_CFG.path_to_train_csv)\n",
        "df_val = pd.read_csv(global_CFG.path_to_validation_csv)\n",
        "\n",
        "def encode_target(df):\n",
        "    df['discourse_effectiveness'] = df['discourse_effectiveness'].map({\n",
        "        'Adequate': 1,\n",
        "        'Effective': 2,\n",
        "        'Ineffective': 0\n",
        "    })\n",
        "    return df\n",
        "\n",
        "df_train = encode_target(df_train)\n",
        "df_val = encode_target(df_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEe3fvS1ahFK",
        "outputId": "1abf3078-79e8-404a-fbbc-b0e2f7134ba1"
      },
      "outputs": [],
      "source": [
        "def add_whole_text(df):\n",
        "    texts = []\n",
        "    for i, data in tqdm(df.iterrows()):\n",
        "        id_ = data['essay_id']\n",
        "        tmp = df[df['essay_id'] == id_]\n",
        "        tmp = tmp['discourse_text'].to_numpy().tolist()\n",
        "        texts.append(''.join(tmp))\n",
        "    df['whole_text'] = texts\n",
        "    return df\n",
        "\n",
        "df_train = add_whole_text(df_train)\n",
        "df_val = add_whole_text(df_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gl_9fhyIiOXA"
      },
      "outputs": [],
      "source": [
        "x_cols = ['discourse_id', 'essay_id', 'discourse_text', 'discourse_type', 'whole_text']\n",
        "y_col = 'discourse_effectiveness'\n",
        "\n",
        "X_train = df_train[x_cols]\n",
        "y_train = df_train[y_col]\n",
        "X_val = df_val[x_cols]\n",
        "y_val = df_val[y_col]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAmWvjYjatFu"
      },
      "outputs": [],
      "source": [
        "class ArgumentsDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.inputs = X['discourse_text']\n",
        "        self.whole_text = X['whole_text']\n",
        "        self.label = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_ = self.inputs[idx] + '[SEP]' + self.whole_text[idx]\n",
        "        if self.label is None:\n",
        "            return input_\n",
        "\n",
        "        label = self.label[idx]\n",
        "        return input_ , label\n",
        "\n",
        "\n",
        "train_dataset = ArgumentsDataset(X_train, y_train)\n",
        "validation_dataset = ArgumentsDataset(X_val, y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgyHlNLHbz3k"
      },
      "outputs": [],
      "source": [
        "class deberta_xsmall_CFG:\n",
        "    model_path = 'microsoft/deberta-v3-xsmall'\n",
        "    trained_model_path = 'checkpoints/microsoft-deberta-v3-xsmall_epoch_9_batchsize_5'\n",
        "    \n",
        "    max_len = 2500\n",
        "    dropout = 0.5\n",
        "\n",
        "    layer_size = 512\n",
        "\n",
        "class DebertaXSmallModel(nn.Module):\n",
        "    def __init__(self, model_name, num_labels=3):\n",
        "        super().__init__()\n",
        "        self.config = AutoConfig.from_pretrained(deberta_xsmall_CFG.model_path)\n",
        "        self.model_name = model_name\n",
        "        self.model = AutoModel.from_pretrained(model_name)\n",
        "        self.dropout = nn.Dropout(deberta_xsmall_CFG.dropout)\n",
        "        self.fc1 = nn.Linear(self.config.hidden_size, deberta_xsmall_CFG.layer_size)\n",
        "        self.fc2 = nn.Linear(deberta_xsmall_CFG.layer_size, num_labels)\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
        "        outputs = self.model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            output_hidden_states=True\n",
        "        )\n",
        "        last_hidden = outputs[0]\n",
        "        last_hidden_sum = last_hidden.mean(dim=1)\n",
        "        \n",
        "        out = torch.tanh(self.fc1(last_hidden_sum))\n",
        "        logits = self.fc2(self.dropout(out))\n",
        "\n",
        "        return {'logits': logits}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swQwp3srWpV3"
      },
      "outputs": [],
      "source": [
        "class deberta_small_CFG:\n",
        "    model_path = 'microsoft/deberta-v3-small'\n",
        "    trained_model_path = 'checkpoints/microsoft-deberta-v3-small_epoch_10_batchsize_5'\n",
        "    \n",
        "    max_len = 1500\n",
        "    dropout = 0.5\n",
        "\n",
        "    layer_size = 256\n",
        "\n",
        "class DebertaSmallModel(nn.Module):\n",
        "    def __init__(self, model_name, num_labels=3):\n",
        "        super().__init__()\n",
        "        self.config = AutoConfig.from_pretrained(deberta_small_CFG.model_path)\n",
        "        self.model_name = model_name\n",
        "        self.model = AutoModel.from_pretrained(model_name)\n",
        "        self.dropout = nn.Dropout(deberta_small_CFG.dropout)\n",
        "        self.fc1 = nn.Linear(self.config.hidden_size, deberta_small_CFG.layer_size)\n",
        "        self.fc2 = nn.Linear(deberta_small_CFG.layer_size, num_labels)\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
        "        outputs = self.model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            output_hidden_states=True\n",
        "        )\n",
        "        last_hidden = outputs[0]\n",
        "        last_hidden_sum = last_hidden.mean(dim=1)\n",
        "        \n",
        "        out = torch.tanh(self.fc1(last_hidden_sum))\n",
        "        logits = self.fc2(self.dropout(out))\n",
        "\n",
        "        return {'logits': logits}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzbyuiqJbDRB"
      },
      "outputs": [],
      "source": [
        "def eval_dataloader(dataloader, model):\n",
        "    cols = ['Adequate', 'Effective', 'Ineffective']\n",
        "    df_new = None\n",
        "    with torch.no_grad():\n",
        "        for j, batch in enumerate(tqdm(dataloader)):\n",
        "            inputs, labels = batch\n",
        "            labels = labels.to(global_CFG.device)\n",
        "            input_ids = inputs['input_ids'].to(global_CFG.device)\n",
        "            attention_mask = inputs['attention_mask'].to(global_CFG.device)\n",
        "            token_type_ids = inputs['token_type_ids'].to(global_CFG.device)\n",
        "\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model.forward(input_ids, attention_mask, token_type_ids)\n",
        "                logits = outputs['logits']\n",
        "            \n",
        "            df_new = logits.cpu().detach().numpy() if df_new is None else np.append(df_new, logits.cpu().detach().numpy(), axis=0)\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    df_new = pd.DataFrame(df_new, columns=cols)\n",
        "    df_new.reset_index(drop=True, inplace=True)\n",
        "    return df_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9UeVHQRbvUp",
        "outputId": "4f305069-3922-4459-ab7e-5bebe8c67efb"
      },
      "outputs": [],
      "source": [
        "def infer_single_model(model_cfg, model_lambda):\n",
        "    checkpoint = torch.load(model_cfg.trained_model_path, map_location=global_CFG.device)\n",
        "    model = model_lambda(model_cfg.model_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
        "    model.to(device=global_CFG.device)\n",
        "    model.eval()\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_cfg.model_path)\n",
        "    max_len = model_cfg.max_len\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=global_CFG.infer_batch_size, shuffle=False, collate_fn=lambda b: collate_fn(b, tokenizer, max_len))\n",
        "    validation_dataloader = DataLoader(validation_dataset, batch_size=global_CFG.infer_batch_size, shuffle=False, collate_fn=lambda b: collate_fn(b, tokenizer, max_len))\n",
        "\n",
        "    validation_infer_df = eval_dataloader(validation_dataloader, model)\n",
        "    train_infer_df = eval_dataloader(train_dataloader, model)\n",
        "    return validation_infer_df, train_infer_df\n",
        "\n",
        "def collate_fn(batch, tokenizer, max_len):\n",
        "    inputs, labels = [], []\n",
        "    for i, l in batch:\n",
        "        inputs.append(i)\n",
        "        labels.append(l)\n",
        "    inputs = tokenizer(\n",
        "        list(inputs), \n",
        "        return_tensors='pt', \n",
        "        truncation=True, \n",
        "        padding=True, \n",
        "        max_length=max_len\n",
        "    )\n",
        "    return inputs, torch.LongTensor(labels)\n",
        "\n",
        "print(f'using {global_CFG.device} . . .')\n",
        "\n",
        "validation_infer_df_xsmall, train_infer_df_xsmall = infer_single_model(deberta_xsmall_CFG, DebertaXSmallModel)\n",
        "validation_infer_df_small, train_infer_df_small   = infer_single_model(deberta_small_CFG, DebertaSmallModel)\n",
        "\n",
        "validation_final_df = global_CFG.w_xsmall * validation_infer_df_xsmall + global_CFG.w_small * validation_infer_df_small  \n",
        "train_final_df      = global_CFG.w_xsmall * train_infer_df_xsmall      + global_CFG.w_small * train_infer_df_small  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uw6FO8XeWpq"
      },
      "outputs": [],
      "source": [
        "display(validation_final_df.head())\n",
        "display(train_final_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhCarKRreYH4"
      },
      "outputs": [],
      "source": [
        "validation_final_df.to_csv('./validation_final_df.csv', index=False)\n",
        "train_final_df.to_csv('./train_final_df.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
